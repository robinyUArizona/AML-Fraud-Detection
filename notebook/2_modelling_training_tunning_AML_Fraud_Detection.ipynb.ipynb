{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, \n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, auc, roc_curve\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load individual train and test dataset arrays\n",
    "train_array = np.load('train_array.npy')\n",
    "test_array = np.load('test_array.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = (\n",
    "    train_array[:, :-1],\n",
    "    train_array[:, -1],\n",
    "    test_array[:, :-1],\n",
    "    test_array[:, -1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.13072593e-01  2.50000000e-05  2.50000000e-05  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 7.32170486e+00  2.50000000e-05  2.50000000e-05  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 6.10589071e-02  2.50000000e-05  2.50000000e-05  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 8.68951820e+00  2.50000000e-05  1.00000000e-04  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   1.00000000e+00]\n",
      " [ 5.01890565e-01  2.50000000e-05  2.50000000e-05  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]]\n",
      "[0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:5])\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
      "[CV] END ............................C=0.1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ............................C=0.1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ................................C=0.1, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ................................C=0.1, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ..............................C=1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..............................C=1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..................................C=1, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ..................................C=1, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .............................C=10, solver=liblinear; total time=   0.0s\n",
      "[CV] END .............................C=10, solver=liblinear; total time=   0.0s\n",
      "[CV] END .................................C=10, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .................................C=10, solver=lbfgs; total time=   0.0s\n",
      "Best Parameters:  {'C': 0.1, 'solver': 'liblinear'}\n",
      "Precision: 1.0\n",
      "Recall: 0.9987\n",
      "F1 score: 0.9993495772251964\n",
      "Confusion Matrix: \n",
      "[[9987   13]\n",
      " [   0    0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],  # Regularization strength\n",
    "    'solver': ['liblinear', 'lbfgs']  # Solvers compatible with smaller datasets\n",
    "}\n",
    "\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=log_reg,\n",
    "    param_grid=param_grid,\n",
    "    cv=2,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on Train data\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "# Predict on Test data\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "print(f\"Precision: {precision_score(y_test_pred, y_test, average='weighted')}\")\n",
    "print(f\"Recall: {recall_score(y_test_pred, y_test, average='weighted')}\")\n",
    "print(f\"F1 score: {f1_score(y_test_pred, y_test, average='weighted')}\")\n",
    "# Compute confusion matrix\n",
    "print(f\"Confusion Matrix: \\n{confusion_matrix(y_test_pred, y_test)}\")\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversamplig the data and cross validating by using different models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  Imbalance dataset - upsampling \"\"\"\n",
    "def upsampling_data(X, y):\n",
    "    sm = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "    X_sm, y_sm = sm.fit_resample(X, y)   \n",
    "    print(f\"\\n====== Upsampled the minority class data ======\") \n",
    "    print(\"Before SMOTE: \", Counter(y))\n",
    "    print(\"After SMOTE: \", Counter(y_sm))\n",
    "    return X_sm, y_sm\n",
    "\n",
    "\n",
    "\"\"\" Evaluating Models \"\"\"\n",
    "def model_evaluation(model, X, y, num_procs):\n",
    "    # parallel cross-validate models\n",
    "    print(\"\\n===================== Beginning cross validation ========================== \")\n",
    "    # Define cross-validation method\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    # Define metrics\n",
    "    metrics = {\n",
    "        'Precision': make_scorer(precision_score, average='weighted'),\n",
    "        'Recall': make_scorer(recall_score, average='weighted'),\n",
    "        'F1 score': make_scorer(f1_score, average='weighted')\n",
    "    }\n",
    "    # Perform cross-validation and evaluate using different metrics\n",
    "    metrics_results = {}\n",
    "    for name, metric in metrics.items():\n",
    "        scores = cross_val_score(model, X, y, cv=cv, scoring=metric).mean()\n",
    "        metrics_results[name] = scores\n",
    "    return metrics_results\n",
    "\n",
    "\n",
    "\"\"\" Calculate Metrics \"\"\"\n",
    "def model_metrics(y_pred, y_test):    \n",
    "    precision = precision_score(y_pred, y_test, average='weighted')\n",
    "    recall = recall_score(y_pred, y_test, average='weighted')\n",
    "    f1 = f1_score(y_pred, y_test, average='weighted')\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_pred, y_test) \n",
    "    return precision, recall, f1, cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Models evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== Upsampled the minority class data ======\n",
      "Before SMOTE:  Counter({np.float64(0.0): 39959, np.float64(1.0): 41})\n",
      "After SMOTE:  Counter({np.float64(0.0): 39959, np.float64(1.0): 39959})\n",
      "\n",
      "================ RandomForestClassifier() ==================================\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[CV] END ....................criterion=gini, n_estimators=50; total time=   2.0s\n",
      "[CV] END ....................criterion=gini, n_estimators=50; total time=   2.1s\n",
      "[CV] END ....................criterion=gini, n_estimators=50; total time=   2.2s\n",
      "[CV] END ....................criterion=gini, n_estimators=50; total time=   2.2s\n",
      "[CV] END ....................criterion=gini, n_estimators=50; total time=   2.2s\n",
      "[CV] END ...................criterion=gini, n_estimators=100; total time=   4.1s\n",
      "[CV] END ...................criterion=gini, n_estimators=100; total time=   4.2s\n",
      "[CV] END ...................criterion=gini, n_estimators=100; total time=   4.2s\n",
      "[CV] END ...................criterion=gini, n_estimators=100; total time=   4.2s\n",
      "[CV] END ...................criterion=gini, n_estimators=100; total time=   4.3s\n",
      "[CV] END .................criterion=entropy, n_estimators=50; total time=   2.4s\n",
      "[CV] END .................criterion=entropy, n_estimators=50; total time=   2.5s\n",
      "[CV] END .................criterion=entropy, n_estimators=50; total time=   2.4s\n",
      "[CV] END .................criterion=entropy, n_estimators=50; total time=   2.4s\n",
      "[CV] END .................criterion=entropy, n_estimators=50; total time=   2.4s\n",
      "[CV] END ...................criterion=gini, n_estimators=200; total time=   8.8s\n",
      "[CV] END ...................criterion=gini, n_estimators=200; total time=   8.7s\n",
      "[CV] END ...................criterion=gini, n_estimators=200; total time=   8.9s\n",
      "[CV] END ...................criterion=gini, n_estimators=200; total time=   9.0s\n",
      "[CV] END ...................criterion=gini, n_estimators=200; total time=   9.0s\n",
      "[CV] END ................criterion=entropy, n_estimators=100; total time=   4.9s\n",
      "[CV] END ................criterion=entropy, n_estimators=100; total time=   4.9s\n",
      "[CV] END ................criterion=entropy, n_estimators=100; total time=   5.0s\n",
      "[CV] END ................criterion=entropy, n_estimators=100; total time=   5.0s\n",
      "[CV] END ................criterion=entropy, n_estimators=100; total time=   5.1s\n",
      "[CV] END ................criterion=entropy, n_estimators=200; total time=   6.6s\n",
      "[CV] END ................criterion=entropy, n_estimators=200; total time=   6.7s\n",
      "[CV] END ................criterion=entropy, n_estimators=200; total time=   6.7s\n",
      "[CV] END ................criterion=entropy, n_estimators=200; total time=   6.7s\n",
      "[CV] END ................criterion=entropy, n_estimators=200; total time=   6.7s\n",
      "Best parameters: {'criterion': 'gini', 'n_estimators': 50} for RandomForestClassifier()\n",
      "Obtaining evaluation metrics for RandomForestClassifier(n_estimators=50) by using best hyperparameters\n",
      "=============================================================\n",
      "\n",
      "\n",
      "\n",
      "================ AdaBoostClassifier() ==================================\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV] END ................learning_rate=0.01, n_estimators=50; total time=   1.7s\n",
      "[CV] END ................learning_rate=0.01, n_estimators=50; total time=   1.8s\n",
      "[CV] END ................learning_rate=0.01, n_estimators=50; total time=   1.8s\n",
      "[CV] END ................learning_rate=0.01, n_estimators=50; total time=   1.8s\n",
      "[CV] END ................learning_rate=0.01, n_estimators=50; total time=   1.8s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=100; total time=   3.4s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=100; total time=   3.5s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=100; total time=   3.5s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=100; total time=   3.5s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=100; total time=   3.6s\n",
      "[CV] END .................learning_rate=0.1, n_estimators=50; total time=   1.8s\n",
      "[CV] END .................learning_rate=0.1, n_estimators=50; total time=   1.7s\n",
      "[CV] END .................learning_rate=0.1, n_estimators=50; total time=   1.8s\n",
      "[CV] END .................learning_rate=0.1, n_estimators=50; total time=   1.8s\n",
      "[CV] END .................learning_rate=0.1, n_estimators=50; total time=   1.7s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=200; total time=   6.8s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=200; total time=   6.8s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=100; total time=   3.3s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=200; total time=   6.9s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=100; total time=   3.5s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=100; total time=   3.3s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=100; total time=   3.4s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=200; total time=   6.9s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=200; total time=   7.0s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=100; total time=   3.5s\n",
      "[CV] END .................learning_rate=0.5, n_estimators=50; total time=   1.7s\n",
      "[CV] END .................learning_rate=0.5, n_estimators=50; total time=   1.7s\n",
      "[CV] END .................learning_rate=0.5, n_estimators=50; total time=   1.8s\n",
      "[CV] END .................learning_rate=0.5, n_estimators=50; total time=   1.8s\n",
      "[CV] END .................learning_rate=0.5, n_estimators=50; total time=   1.8s\n",
      "[CV] END ................learning_rate=0.5, n_estimators=100; total time=   3.3s\n",
      "[CV] END ................learning_rate=0.5, n_estimators=100; total time=   3.2s\n",
      "[CV] END ................learning_rate=0.5, n_estimators=100; total time=   3.3s\n",
      "[CV] END ................learning_rate=0.5, n_estimators=100; total time=   3.4s\n",
      "[CV] END ................learning_rate=0.5, n_estimators=100; total time=   3.3s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=200; total time=   6.7s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=200; total time=   6.8s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=200; total time=   6.9s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=200; total time=   6.8s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=200; total time=   6.9s\n",
      "[CV] END .................learning_rate=1.0, n_estimators=50; total time=   1.8s\n",
      "[CV] END .................learning_rate=1.0, n_estimators=50; total time=   1.8s\n",
      "[CV] END .................learning_rate=1.0, n_estimators=50; total time=   1.8s\n",
      "[CV] END .................learning_rate=1.0, n_estimators=50; total time=   1.8s\n",
      "[CV] END .................learning_rate=1.0, n_estimators=50; total time=   1.8s\n",
      "[CV] END ................learning_rate=1.0, n_estimators=100; total time=   3.3s\n",
      "[CV] END ................learning_rate=1.0, n_estimators=100; total time=   3.3s\n",
      "[CV] END ................learning_rate=0.5, n_estimators=200; total time=   6.8s\n",
      "[CV] END ................learning_rate=1.0, n_estimators=100; total time=   3.5s\n",
      "[CV] END ................learning_rate=1.0, n_estimators=100; total time=   3.3s\n",
      "[CV] END ................learning_rate=0.5, n_estimators=200; total time=   6.9s\n",
      "[CV] END ................learning_rate=1.0, n_estimators=100; total time=   3.4s\n",
      "[CV] END ................learning_rate=0.5, n_estimators=200; total time=   6.9s\n",
      "[CV] END ................learning_rate=0.5, n_estimators=200; total time=   7.0s\n",
      "[CV] END ................learning_rate=0.5, n_estimators=200; total time=   7.0s\n",
      "[CV] END ................learning_rate=1.0, n_estimators=200; total time=   4.4s\n",
      "[CV] END ................learning_rate=1.0, n_estimators=200; total time=   4.5s\n",
      "[CV] END ................learning_rate=1.0, n_estimators=200; total time=   4.4s\n",
      "[CV] END ................learning_rate=1.0, n_estimators=200; total time=   4.4s\n",
      "[CV] END ................learning_rate=1.0, n_estimators=200; total time=   4.5s\n",
      "Best parameters: {'learning_rate': 1.0, 'n_estimators': 200} for AdaBoostClassifier()\n",
      "Obtaining evaluation metrics for AdaBoostClassifier(n_estimators=200) by using best hyperparameters\n",
      "=============================================================\n",
      "\n",
      "\n",
      "\n",
      "================ XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) ==================================\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV] END ................learning_rate=0.01, n_estimators=50; total time=   0.2s\n",
      "[CV] END ................learning_rate=0.01, n_estimators=50; total time=   0.2s\n",
      "[CV] END ................learning_rate=0.01, n_estimators=50; total time=   0.2s\n",
      "[CV] END ................learning_rate=0.01, n_estimators=50; total time=   0.2s\n",
      "[CV] END ................learning_rate=0.01, n_estimators=50; total time=   0.2s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=100; total time=   0.3s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=100; total time=   0.4s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=100; total time=   0.4s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=100; total time=   0.4s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=100; total time=   0.4s\n",
      "[CV] END .................learning_rate=0.1, n_estimators=50; total time=   0.3s\n",
      "[CV] END .................learning_rate=0.1, n_estimators=50; total time=   0.2s\n",
      "[CV] END .................learning_rate=0.1, n_estimators=50; total time=   0.3s\n",
      "[CV] END .................learning_rate=0.1, n_estimators=50; total time=   0.3s\n",
      "[CV] END .................learning_rate=0.1, n_estimators=50; total time=   0.3s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=200; total time=   0.7s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=200; total time=   0.7s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=200; total time=   0.7s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=200; total time=   0.7s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=200; total time=   0.7s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=100; total time=   0.4s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=100; total time=   0.4s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=100; total time=   0.4s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=100; total time=   0.4s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=100; total time=   0.4s\n",
      "[CV] END ................learning_rate=0.05, n_estimators=50; total time=   0.2s\n",
      "[CV] END ................learning_rate=0.05, n_estimators=50; total time=   0.2s\n",
      "[CV] END ................learning_rate=0.05, n_estimators=50; total time=   0.2s\n",
      "[CV] END ................learning_rate=0.05, n_estimators=50; total time=   0.2s\n",
      "[CV] END ................learning_rate=0.05, n_estimators=50; total time=   0.2s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=200; total time=   0.7s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=200; total time=   0.7s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=200; total time=   0.7s\n",
      "[CV] END ...............learning_rate=0.05, n_estimators=100; total time=   0.4s\n",
      "[CV] END ...............learning_rate=0.05, n_estimators=100; total time=   0.4s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=200; total time=   0.7s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=200; total time=   0.7s\n",
      "[CV] END ...............learning_rate=0.05, n_estimators=100; total time=   0.4s\n",
      "[CV] END ...............learning_rate=0.05, n_estimators=100; total time=   0.3s\n",
      "[CV] END ...............learning_rate=0.05, n_estimators=100; total time=   0.4s\n",
      "[CV] END ...............learning_rate=0.001, n_estimators=50; total time=   0.2s\n",
      "[CV] END ...............learning_rate=0.001, n_estimators=50; total time=   0.2s\n",
      "[CV] END ...............learning_rate=0.001, n_estimators=50; total time=   0.2s\n",
      "[CV] END ...............learning_rate=0.001, n_estimators=50; total time=   0.2s\n",
      "[CV] END ...............learning_rate=0.001, n_estimators=50; total time=   0.2s\n",
      "[CV] END ..............learning_rate=0.001, n_estimators=100; total time=   0.3s\n",
      "[CV] END ..............learning_rate=0.001, n_estimators=100; total time=   0.3s\n",
      "[CV] END ...............learning_rate=0.05, n_estimators=200; total time=   0.7s\n",
      "[CV] END ..............learning_rate=0.001, n_estimators=100; total time=   0.4s\n",
      "[CV] END ...............learning_rate=0.05, n_estimators=200; total time=   0.7s\n",
      "[CV] END ...............learning_rate=0.05, n_estimators=200; total time=   0.7s\n",
      "[CV] END ...............learning_rate=0.05, n_estimators=200; total time=   0.7s\n",
      "[CV] END ...............learning_rate=0.05, n_estimators=200; total time=   0.7s\n",
      "[CV] END ..............learning_rate=0.001, n_estimators=100; total time=   0.4s\n",
      "[CV] END ..............learning_rate=0.001, n_estimators=100; total time=   0.4s\n",
      "[CV] END ..............learning_rate=0.001, n_estimators=200; total time=   0.5s\n",
      "[CV] END ..............learning_rate=0.001, n_estimators=200; total time=   0.5s\n",
      "[CV] END ..............learning_rate=0.001, n_estimators=200; total time=   0.5s\n",
      "[CV] END ..............learning_rate=0.001, n_estimators=200; total time=   0.5s\n",
      "[CV] END ..............learning_rate=0.001, n_estimators=200; total time=   0.5s\n",
      "Best parameters: {'learning_rate': 0.1, 'n_estimators': 200} for XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...)\n",
      "Obtaining evaluation metrics for XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=200, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) by using best hyperparameters\n",
      "=============================================================\n",
      "\n",
      "\n",
      "Model performance on Training Data: \n",
      "{'Random_Forest': [{'Precision': 0.9998999086679278, 'Recall': 0.9998998973948297, 'F1 score': 0.999899897395394, 'Confusion Matrix': array([[39952,     1],\n",
      "       [    7, 39958]])}], 'AdaBoost': [{'Precision': 0.9684769752932288, 'Recall': 0.9674040891914212, 'F1 score': 0.9674215844264595, 'Confusion Matrix': array([[37731,   377],\n",
      "       [ 2228, 39582]])}], 'XGBoost': [{'Precision': 0.9906379325978477, 'Recall': 0.9905027653344678, 'F1 score': 0.9905034072354595, 'Confusion Matrix': array([[39251,    51],\n",
      "       [  708, 39908]])}]}\n",
      "-----------------------------------------------------\n",
      "Model performance on Test Data: \n",
      "{'Random_Forest': [{'Precision': 0.9618226432824211, 'Recall': 0.9788, 'F1 score': 0.9695885681464568, 'Confusion Matrix': array([[9787,   12],\n",
      "       [ 200,    1]])}], 'AdaBoost': [{'Precision': 0.9218786098851584, 'Recall': 0.9371, 'F1 score': 0.9079230021968008, 'Confusion Matrix': array([[9362,    4],\n",
      "       [ 625,    9]])}], 'XGBoost': [{'Precision': 0.9574942194083078, 'Recall': 0.9765, 'F1 score': 0.9661644786073622, 'Confusion Matrix': array([[9764,   12],\n",
      "       [ 223,    1]])}]}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Evaluating Models\n",
    "    - Random_Forest\n",
    "    - AdaBoost\n",
    "    - XGBoost\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the classifiers\n",
    "models = {\n",
    "    \"Random_Forest\": RandomForestClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    # \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"XGBoost\": XGBClassifier()\n",
    "}\n",
    "\n",
    "\n",
    "# Define the hyperparameters and their possible values\n",
    "params = {\n",
    "    \"Random_Forest\": {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        # 'max_depth': [None, 10, 20, 30],\n",
    "        # 'min_samples_split': [2, 5, 10],\n",
    "        # 'min_samples_leaf': [1, 2, 4],\n",
    "        # 'max_features': ['sqrt', 'log2', None]\n",
    "    },\n",
    "    \"AdaBoost\": {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.5, 1.0],\n",
    "        # 'algorithm': ['SAMME', 'SAMME.R']\n",
    "    },\n",
    "    # \"Gradient Boosting\": {\n",
    "    #     # 'n_estimators': [50, 100, 200],\n",
    "    #     # 'learning_rate': [0.01, 0.1, 0.05, 0.001],\n",
    "    #     # 'subsample': [0.6, 0.7, 0.8, 0.9],\n",
    "    #     # 'max_depth': [3, 5, 7, 9],\n",
    "    #     # 'min_samples_split': [2, 5, 10],\n",
    "    #     # 'min_samples_leaf': [1, 2, 4]\n",
    "    # },\n",
    "    \"XGBoost\": {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.05, 0.001],\n",
    "        # 'max_depth': [3, 5, 7, 9],\n",
    "        # 'min_child_weight': [1, 3, 5],\n",
    "        # 'gamma': [0, 0.1, 0.2],\n",
    "        # 'subsample': [0.6, 0.7, 0.8, 0.9],\n",
    "        # 'colsample_bytree': [0.6, 0.7, 0.8, 0.9]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize StratifiedKFold with 5 folds\n",
    "# Stratified K-Fold ensures that each fold has the same proportion of classes as the entire dataset. \n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Upsampling train data\n",
    "X_train_sm, y_train_sm = upsampling_data(X_train, y_train)\n",
    "\n",
    "train_report = {}\n",
    "test_report = {}\n",
    "for i in range(len(models)):\n",
    "    model = list(models.values())[i]\n",
    "    param = params[list(models.keys())[i]]\n",
    "    print(f\"\\n================ {model} ==================================\")\n",
    "    # Initalize GridSearchCV with 5-fold stratified cross validation\n",
    "    # n_jobs is set to -1, to use all available CPU cores on the machine\n",
    "    # verbose=2 to track the progress grid search or model training.\n",
    "    grid_search = GridSearchCV(estimator=model, \n",
    "                               param_grid=param, \n",
    "                               cv=skf,\n",
    "                               n_jobs=-1,\n",
    "                               verbose=2)\n",
    "\n",
    "    # Fit the grid search model\n",
    "    grid_search.fit(X_train_sm, y_train_sm)\n",
    "    print(f\"Best parameters: {grid_search.best_params_} for {model}\")\n",
    "\n",
    "    # Set the model with best hyperparameters\n",
    "    model.set_params(**grid_search.best_params_)\n",
    "    # Fit the model\n",
    "    model.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "    # Predict on Train data\n",
    "    y_train_pred = model.predict(X_train_sm)\n",
    "    # Predict on Test data\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Get evaluation metrics for train and test data using best hyperparametrs\n",
    "    print(f\"Obtaining evaluation metrics for {model} by using best hyperparameters\")\n",
    "    precision_train, recall_train, f1_train, cm_train = model_metrics(y_train_pred, y_train_sm)\n",
    "    train_model_score = []\n",
    "    train_model_score.append({\n",
    "        \"Precision\" : precision_train,\n",
    "        \"Recall\" : recall_train,\n",
    "        \"F1 score\": f1_train,\n",
    "        \"Confusion Matrix\": cm_train\n",
    "    })\n",
    "    train_report[list(models.keys())[i]] = train_model_score\n",
    "\n",
    "    precision_test, recall_test, f1_test, cm_test = model_metrics(y_test_pred, y_test)\n",
    "    test_model_score = []\n",
    "    test_model_score.append({\n",
    "        \"Precision\" : precision_test,\n",
    "        \"Recall\" : recall_test,\n",
    "        \"F1 score\": f1_test,\n",
    "        \"Confusion Matrix\": cm_test\n",
    "    })\n",
    "    test_report[list(models.keys())[i]] = test_model_score\n",
    "\n",
    "    print(\"=============================================================\\n\\n\")\n",
    "\n",
    "print(f\"Model performance on Training Data: \\n{train_report}\")\n",
    "print(f\"-----------------------------------------------------\")\n",
    "print(f\"Model performance on Test Data: \\n{test_report}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models evaluation results on training data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance on Train data: \n",
      "           Model  Precision    Recall  F1 Score               Confusion Matrix\n",
      "0  Random_Forest   0.999900  0.999900  0.999900       [[39952, 1], [7, 39958]]\n",
      "1       AdaBoost   0.968477  0.967404  0.967422  [[37731, 377], [2228, 39582]]\n",
      "2        XGBoost   0.990638  0.990503  0.990503    [[39251, 51], [708, 39908]]\n",
      "\n",
      "Model performance on Test data: \n",
      "           Model  Precision  Recall  F1 Score        Confusion Matrix\n",
      "0  Random_Forest   0.961823  0.9788  0.969589  [[9787, 12], [200, 1]]\n",
      "1       AdaBoost   0.921879  0.9371  0.907923   [[9362, 4], [625, 9]]\n",
      "2        XGBoost   0.957494  0.9765  0.966164  [[9764, 12], [223, 1]]\n"
     ]
    }
   ],
   "source": [
    "train_report_df = pd.DataFrame({\n",
    "    'Model': list(train_report.keys()),\n",
    "    'Precision': [metrics[0]['Precision'] for metrics in train_report.values()],\n",
    "    'Recall': [metrics[0]['Recall'] for metrics in train_report.values()],\n",
    "    'F1 Score': [metrics[0]['F1 score'] for metrics in train_report.values()],\n",
    "    'Confusion Matrix': [metrics[0]['Confusion Matrix'] for metrics in train_report.values()]\n",
    "})\n",
    "print(f\"Model performance on Train data: \\n{train_report_df}\\n\")\n",
    "\n",
    "test_report_df = pd.DataFrame({\n",
    "    'Model': list(test_report.keys()),\n",
    "    'Precision': [metrics[0]['Precision'] for metrics in test_report.values()],\n",
    "    'Recall': [metrics[0]['Recall'] for metrics in test_report.values()],\n",
    "    'F1 Score': [metrics[0]['F1 score'] for metrics in test_report.values()],\n",
    "    'Confusion Matrix': [metrics[0]['Confusion Matrix'] for metrics in test_report.values()]\n",
    "})\n",
    "print(f\"Model performance on Test data: \\n{test_report_df}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Random_Forest': [{'Precision': 0.9618226432824211,\n",
       "   'Recall': 0.9788,\n",
       "   'F1 score': 0.9695885681464568,\n",
       "   'Confusion Matrix': array([[9787,   12],\n",
       "          [ 200,    1]])}],\n",
       " 'AdaBoost': [{'Precision': 0.9218786098851584,\n",
       "   'Recall': 0.9371,\n",
       "   'F1 score': 0.9079230021968008,\n",
       "   'Confusion Matrix': array([[9362,    4],\n",
       "          [ 625,    9]])}],\n",
       " 'XGBoost': [{'Precision': 0.9574942194083078,\n",
       "   'Recall': 0.9765,\n",
       "   'F1 score': 0.9661644786073622,\n",
       "   'Confusion Matrix': array([[9764,   12],\n",
       "          [ 223,    1]])}]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models and its corresponding Recall score from test training report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The models and their corresponding Recall score: \n",
      "{'Random_Forest': 0.9788, 'AdaBoost': 0.9371, 'XGBoost': 0.9765}\n"
     ]
    }
   ],
   "source": [
    "models_recall_score = {model: recall_result[0][\"Recall\"] for model, recall_result in test_report.items()}\n",
    "print(f\"The models and their corresponding Recall score: \\n{models_recall_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the best model and its score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model name is Random_Forest with Recall score: 0.9788\n",
      "Best Model: RandomForestClassifier(n_estimators=50)\n"
     ]
    }
   ],
   "source": [
    "best_model_name, best_score = max(models_recall_score.items(), key=lambda item: item[1])\n",
    "print(f\"Best Model name is {best_model_name} with Recall score: {best_score}\")\n",
    "\n",
    "# Get best model\n",
    "best_model = models[best_model_name]\n",
    "print(f\"Best Model: {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the the best model \n",
    "best_model = RandomForestClassifier(n_estimators=200)\n",
    "# Train the model on the training data\n",
    "rfc_best = best_model.fit(X_train_sm, y_train_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of RandomForestClassifier(n_estimators=200)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_best.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Predict the labels for the test set\n",
    "y_pred = rfc_best.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Final Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random_Forest</td>\n",
       "      <td>0.957494</td>\n",
       "      <td>0.9765</td>\n",
       "      <td>0.966164</td>\n",
       "      <td>[[9764, 12], [223, 1]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Final Model  Precision  Recall  F1 score        Confusion Matrix\n",
       "0  Random_Forest   0.957494  0.9765  0.966164  [[9764, 12], [223, 1]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_train, recall_train, f1_train, cm_train = model_metrics(y_pred, y_test)\n",
    "result_dict = {\"Final Model\": best_model_name,\n",
    "               \"Precision\" : precision_test,\n",
    "               \"Recall\" : recall_test,\n",
    "               \"F1 score\": f1_test,\n",
    "               \"Confusion Matrix\": cm_test}\n",
    "final_results = pd.DataFrame([result_dict])\n",
    "final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict the probabilities for the test set\n",
    "# test_probabilities = rfc_best.predict_proba(X_test)[:, 1]\n",
    "# print(f\"Probabilities prediction of 'Is Laundering':\\n {test_probabilities}\")\n",
    "\n",
    "# # Compute the ROC curve and AUC\n",
    "# fpr, tpr, thresholds = roc_curve(test_probabilities, y_test)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# # Plot the ROC curve\n",
    "# plt.figure(figsize=(4,4))\n",
    "# RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name=rfc_best).plot()\n",
    "# plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')\n",
    "# plt.title('ROC Curve')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the True Positive Rate (TPR) and False Positive Rate (FPR) for a specific threshold value\n",
    "# desired_tpr = 0.88\n",
    "# closest_threshold = thresholds[np.argmin(np.abs(tpr - desired_tpr))]\n",
    "# print(f\"Closet threshold to get Desired TPR of around {desired_tpr*100}%: {closest_threshold}\")\n",
    "\n",
    "# y_pred = (test_probabilities >= closest_threshold).astype(int)\n",
    "# tn, fp, fn, tp = confusion_matrix(y_pred, y_test).ravel()\n",
    "\n",
    "# fpr_cm = fp / (fp + tn)\n",
    "# tpr_cm = tp / (tp + fn)\n",
    "# print(f\"False Positive Rate (FPR): {fpr_cm:.3f}\")\n",
    "# print(f\"True Positive Rate (TPR): {tpr_cm:.3f}\")\n",
    "\n",
    "# disp = ConfusionMatrixDisplay.from_predictions(y_pred, y_test, cmap=\"Blues\")\n",
    "# plt.show()\n",
    "\n",
    "# print(f\"Classification report: \\n {classification_report(y_pred, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best model is Random Forest with Recall (True Postive Rate) of ~99%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
